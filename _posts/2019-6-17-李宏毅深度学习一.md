---
published: true
title: 李宏毅深度学习课程: Why Deep？
category: Machine learning
tags: 
  - ML
  - DL
  - NLP
layout: post
---

# Three Steps for Deep Learning

如果你学过机器学习，知道什么是 **model** 和**goodness**,那么其实你也懂深度学习。

回顾机器学习，我们接触到的第一个模型大概会是 **linear** **model**，确定模型之后，再决定损失函数长什么样，确定data set，用梯度下降等方法去优化此损失函数，令lost最小，最后得到一个参数确定的函数，这个函数就是训练的成果。

所以我们可以将机器学习分为三部曲：

1. model
2. goodness of function
3. pick the best function (optimization) 

在深度学习中，其实就只是改变了第一步的model而已，机器学习的model是 linear model, logistic regression, svm

等，深度学习的model是各种neural network，仅此而已。

# What is Neural Network?

要学习神经网络，不如先来看看 **fully connect feedforward network** 长什么样。

![fully connect feedforward network](https://raw.githubusercontent.com/Logos23333/Logos23333.github.io/master/_posts/image/ml/fully connect feedforward network.png) 

整个fully connect feedforward network可以分成四个部分：input layer、hidden layers、output layer 和 output。

hidden layers可以有很多层，每一个layer可以有很多neurons，具体的层数和神经元数可以自己去设计，当你确定好这个network的结构时，model就已经确定好了，这里的model也可以表示李老师所说的"function set"。之所以叫fully connect feedforward network，是因为在这个network中，前一层和后一层是全连接的，即前一层的全部神经元与后一层的每个神经元都进行了连接，并且此network是一层一层向前推进的，所以是feedforward（？关于feedforward的解释是笔者猜想）。

# Why Deep？

李老师在ppt里给了一个很有趣的定理： **Univerality Theorem**。

![fully connect feedforward network](https://raw.githubusercontent.com/Logos23333/Logos23333.github.io/master/_posts/image/ml/universality theorem.png)

关于此定理，可以做以下阐述：只要神经元的数量足够多，那么只用一层hidden layer就能实现任何的网络结构。